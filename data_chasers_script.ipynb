{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts txt to csv files\n",
    "import csv \n",
    "\n",
    "def txt_to_csv(txt_file):\n",
    "    csv_file = txt_file.split('/')[-1].split('.')[0] + '.csv'\n",
    "    csv_file = '/'.join(txt_file.split('/')[:-1]) + '/' + csv_file\n",
    "    \n",
    "    inp_data = list()\n",
    "    with open(txt_file, 'r') as in_file:\n",
    "        stripped = (line.strip() for line in in_file)\n",
    "        inp_data = [line.split(\"\\t\") for line in stripped if line]\n",
    "    \n",
    "    # creating dataframe\n",
    "    df = pd.DataFrame(inp_data[1:-2], columns=inp_data[0])\n",
    "    print(df.head())\n",
    "    df.to_csv(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data\n",
    "crime_data = pd.read_csv(\"../../../../Downloads/Crimes_-_2001_to_present.csv\")\n",
    "analysis_df = crime_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis harness\n",
    "import pandas as pd\n",
    "\n",
    "# To show different metrics\n",
    "# analysis_df.info()\n",
    "# analysis_df['Arrest'].describe()\n",
    "\n",
    "# import numpy as np\n",
    "# from matplotlib import cm\n",
    "# color = cm.inferno_r(np.linspace(.4,.8, 30))\n",
    "\n",
    "# analysis_df.groupby(['Primary Type', 'Arrest']).size().unstack().plot(kind='barh', stacked=True, figsize=(10,10))\n",
    "# df2 = analysis_df.groupby(['Primary Type', 'Arrest'])['Primary Type'].count().unstack('Arrest').fillna(0)\n",
    "# print (df2.head())\n",
    "# df2[[False,True]].plot(kind='bar', stacked=True)\n",
    "\n",
    "# analysis_df['Primary Type'].value_counts().plot(kind='barh', figsize=(10,10), color=color)\n",
    "# analysis_df.isnull().sum().plot(kind='barh', figsize=(10,10), color=color)\n",
    "# analysis_df[analysis_df['Arrest']==False]['Primary Type'].value_counts().plot(kind='pie', figsize=(10,10))\n",
    "# df = analysis_df[analysis_df['Arrest']==False]['Primary Type'].value_counts()\n",
    "# python_pie3D(df.data, df.index)\n",
    "\n",
    "# To get null values\n",
    "# analysis_df.isnull().sum()\n",
    "\n",
    "# Finding co-relation\n",
    "# correlation_matrix = analysis_df.corr().round(2)\n",
    "# sns.heatmap(data=correlation_matrix, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REST API Client\n",
    "import requests\n",
    "\n",
    "class API_client:\n",
    "    def __init__(self, url, username, password):\n",
    "        self._url = url\n",
    "        self._auth = (username, password)\n",
    "        \n",
    "    def set_url(self, url):\n",
    "        '''\n",
    "        to reset url\n",
    "        '''\n",
    "        self._url = url\n",
    "        \n",
    "    def get_response(self):\n",
    "        '''\n",
    "        get response from url\n",
    "        '''\n",
    "        resp = requests.get(self._url, auth=self._auth)\n",
    "        \n",
    "        if resp.status_code != 200:\n",
    "            # This means something went wrong.\n",
    "            raise ApiError('GET rest API issue: {}'.format(resp.status_code))\n",
    "            \n",
    "        return resp.json()\n",
    "    \n",
    "    def put_response(self, data):\n",
    "        '''\n",
    "        put response\n",
    "        '''\n",
    "        resp = requests.post(self._url, auth=self._auth, json=data)\n",
    "        \n",
    "        if resp.status_code != 201:\n",
    "            raise ApiError('POST rest API issue: {}'.format(resp.status_code))\n",
    "            \n",
    "        return 'POST successful'\n",
    "        \n",
    "    def __str__(self):\n",
    "        return 'This is a REST API functions wrapper'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# analysis_df = analysis_df.to_json(orient='index')\n",
    "host_name = 'http://localhost:8091'\n",
    "rest_client = API_client(couchbase_url, 'administrator', 'nainyjain')\n",
    "\n",
    "# Exploration\n",
    "url_1 = '/pools/default/buckets'\n",
    "\n",
    "rest_client.set_url(host_name+url_1)\n",
    "print (rest_client.get_response())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data\n",
    "import pandas as pd\n",
    "crime_data = pd.read_csv(\"../../../../Downloads/Crimes_-_2001_to_present.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserting data\n",
      "Sequential Timing:  3.682948\n"
     ]
    }
   ],
   "source": [
    "# Data Ingestion\n",
    "\n",
    "# Global connection objects\n",
    "import json\n",
    "import datetime\n",
    "from couchbase.cluster import Cluster\n",
    "from couchbase.cluster import PasswordAuthenticator\n",
    "\n",
    "# Global DB connection objects\n",
    "cluster = Cluster('couchbase://34.229.250.146')\n",
    "authenticator = PasswordAuthenticator('admin', 'password')\n",
    "cluster.authenticate(authenticator)\n",
    "cb = cluster.open_bucket('crime_data')\n",
    "\n",
    "# Sequential run\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "print ('Inserting data')\n",
    "for index, doc in enumerate(crime_data[:2500].T.to_dict().values()):\n",
    "    doc_id = 'k' + str(index)\n",
    "    cb.upsert(doc_id, doc)\n",
    "    \n",
    "end_time = datetime.datetime.now()\n",
    "print ('Sequential Timing: ', (end_time - start_time).total_seconds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to connect to DB and load data\n",
    "def push_data(data, cb, thread_name):\n",
    "    '''\n",
    "    Push data using parallel threads\n",
    "    '''\n",
    "    # Looping and inserting data\n",
    "    for index, doc in enumerate(data):\n",
    "        doc_id = 'k' + str(index)\n",
    "        cb.upsert(doc_id, doc)\n",
    "        \n",
    "    print ('Thread completed insertion: ', thread_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "_TimeoutError_0x17 (generated, catch TimeoutError)",
     "evalue": "<Key='k0', RC=0x17[Client-Side timeout exceeded for operation. Inspect network conditions or increase the timeout], Operational Error, Results=1, C Source=(src/multiresult.c,316), Tracing Output={\"k0\": {\"s\": \"kv:Unknown\", \"i\": 2016389708, \"b\": \"crime_data\", \"r\": \"172.17.0.2:11210\", \"t\": 2500000}}>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_TimeoutError_0x17 (generated, catch TimeoutError)\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-fdb4924c1250>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrime_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mthread\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthreading\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mThread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpush_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Thread:'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mjobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthread\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-42-504d2c796c13>\u001b[0m in \u001b[0;36mpush_data\u001b[0;34m(data, cb, thread_name)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mdoc_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'k'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mcb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'Thread completed insertion: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthread_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/env/lib/python3.7/site-packages/couchbase/bucket.py\u001b[0m in \u001b[0;36mupsert\u001b[0;34m(self, key, value, cas, ttl, format, persist_to, replicate_to)\u001b[0m\n\u001b[1;32m    412\u001b[0m         return _Base.upsert(self, key, value, cas=cas, ttl=ttl,\n\u001b[1;32m    413\u001b[0m                             \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpersist_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpersist_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m                             replicate_to=replicate_to)\n\u001b[0m\u001b[1;32m    415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mttl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpersist_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplicate_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31m_TimeoutError_0x17 (generated, catch TimeoutError)\u001b[0m: <Key='k0', RC=0x17[Client-Side timeout exceeded for operation. Inspect network conditions or increase the timeout], Operational Error, Results=1, C Source=(src/multiresult.c,316), Tracing Output={\"k0\": {\"s\": \"kv:Unknown\", \"i\": 2016389708, \"b\": \"crime_data\", \"r\": \"172.17.0.2:11210\", \"t\": 2500000}}>"
     ]
    }
   ],
   "source": [
    "# Client code for data insertion\n",
    "import threading\n",
    "\n",
    "threads = 2\n",
    "chunk_size = 10\n",
    "jobs = []\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "for i in range(0, threads):\n",
    "    data = crime_data[chunk_size*i:chunk_size*(i+1)].T.to_dict().values()\n",
    "    thread = threading.Thread(target=push_data(data, cb, 'Thread:'+str(i+1)))\n",
    "    jobs.append(thread)\n",
    "\n",
    "# Start the threads (i.e. calculate the random number lists)\n",
    "for j in jobs:\n",
    "    j.start()\n",
    "\n",
    "# Ensure all of the threads have finished\n",
    "for j in jobs:\n",
    "    j.join()\n",
    "\n",
    "print (\"List processing complete.\")\n",
    "end_time = datetime.datetime.now()\n",
    "print ('Parallel Threading Time with global connection object: ', (end_time - start_time).total_seconds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
